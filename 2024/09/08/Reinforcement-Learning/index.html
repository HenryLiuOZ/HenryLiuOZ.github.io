<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Reinforcement Learning | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="æ­¤åšå®¢ä¸»è¦ä»‹ç»å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç›¸å…³è®ºæ–‡ä¹Ÿä¼šæ”¾åœ¨é‡Œé¢ Chapter1ï¼šå¼ºåŒ–å­¦ä¹ èƒŒæ™¯ä¸åŸºç¡€çŸ¥è¯†å‚è€ƒï¼šhuggingfaceWindyLab å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ—¨åœ¨è§£å†³ä¸€ç±»é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (é©¬å°”å¯å¤«æ€§ï¼šç¯å¢ƒçŠ¶æ€è½¬ç§»çš„æ¦‚ç‡ä»…ä¾èµ–äºå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œ) çš„é—®é¢˜ã€‚å¼ºåŒ–å­¦ä¹ çš„å‡è®¾æ˜¯ä»£ç†èƒ½å¤Ÿåœ¨ä¸€ä¸ªé™æ€ç¯å¢ƒ environment with stationary distribution ï¼ˆçŠ¶æ€è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°æ˜¯æ’å®š">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning">
<meta property="og:url" content="http://example.com/2024/09/08/Reinforcement-Learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="æ­¤åšå®¢ä¸»è¦ä»‹ç»å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç›¸å…³è®ºæ–‡ä¹Ÿä¼šæ”¾åœ¨é‡Œé¢ Chapter1ï¼šå¼ºåŒ–å­¦ä¹ èƒŒæ™¯ä¸åŸºç¡€çŸ¥è¯†å‚è€ƒï¼šhuggingfaceWindyLab å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ—¨åœ¨è§£å†³ä¸€ç±»é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (é©¬å°”å¯å¤«æ€§ï¼šç¯å¢ƒçŠ¶æ€è½¬ç§»çš„æ¦‚ç‡ä»…ä¾èµ–äºå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œ) çš„é—®é¢˜ã€‚å¼ºåŒ–å­¦ä¹ çš„å‡è®¾æ˜¯ä»£ç†èƒ½å¤Ÿåœ¨ä¸€ä¸ªé™æ€ç¯å¢ƒ environment with stationary distribution ï¼ˆçŠ¶æ€è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°æ˜¯æ’å®š">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/MARL/Rint_define.png">
<meta property="og:image" content="http://example.com/images/MARL/ELLMprocess.png">
<meta property="og:image" content="http://example.com/images/Multi-Agents/Eureka1.png">
<meta property="og:image" content="http://example.com/images/MARL/soccer2v3.png">
<meta property="og:image" content="http://example.com/images/MARL/behavior_model.png">
<meta property="og:image" content="http://example.com/images/MARL/related_work.png">
<meta property="og:image" content="http://example.com/images/MARL/high_level_RL.png">
<meta property="og:image" content="http://example.com/images/MARL/OCB_reward.png">
<meta property="og:image" content="http://example.com/images/MARL/QMIX.png">
<meta property="og:image" content="http://example.com/images/MARL/Qtot.png">
<meta property="og:image" content="http://example.com/images/MARL/MADDPG.png">
<meta property="og:image" content="http://example.com/images/MARL/COMA.png">
<meta property="og:image" content="http://example.com/images/MARL/Advantage_function.png">
<meta property="og:image" content="http://example.com/images/MARL/COMA_policy_update.png">
<meta property="og:image" content="http://example.com/images/MARL/POCA_center_critic.png">
<meta property="og:image" content="http://example.com/images/MARL/POCA_advantage.png">
<meta property="og:image" content="http://example.com/images/MARL/co_pass.png">
<meta property="article:published_time" content="2024-09-08T09:05:10.000Z">
<meta property="article:modified_time" content="2025-03-10T06:10:52.095Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/MARL/Rint_define.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/09/08/Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2024-09-08T09:05:10.000Z" itemprop="datePublished">2024-09-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Reinforcement Learning
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>æ­¤åšå®¢ä¸»è¦ä»‹ç»å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç›¸å…³è®ºæ–‡ä¹Ÿä¼šæ”¾åœ¨é‡Œé¢</p>
<h2 id="Chapter1ï¼šå¼ºåŒ–å­¦ä¹ èƒŒæ™¯ä¸åŸºç¡€çŸ¥è¯†"><a href="#Chapter1ï¼šå¼ºåŒ–å­¦ä¹ èƒŒæ™¯ä¸åŸºç¡€çŸ¥è¯†" class="headerlink" title="Chapter1ï¼šå¼ºåŒ–å­¦ä¹ èƒŒæ™¯ä¸åŸºç¡€çŸ¥è¯†"></a>Chapter1ï¼šå¼ºåŒ–å­¦ä¹ èƒŒæ™¯ä¸åŸºç¡€çŸ¥è¯†</h2><p>å‚è€ƒï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit1/what-is-rl">huggingface</a><br><a target="_blank" rel="noopener" href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning">WindyLab</a></p>
<p>å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ—¨åœ¨è§£å†³ä¸€ç±»é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (é©¬å°”å¯å¤«æ€§ï¼šç¯å¢ƒçŠ¶æ€è½¬ç§»çš„æ¦‚ç‡ä»…ä¾èµ–äºå½“å‰çŠ¶æ€å’ŒåŠ¨ä½œ) çš„é—®é¢˜ã€‚å¼ºåŒ–å­¦ä¹ çš„å‡è®¾æ˜¯ä»£ç†èƒ½å¤Ÿåœ¨ä¸€ä¸ªé™æ€ç¯å¢ƒ <em>environment with stationary distribution</em> ï¼ˆçŠ¶æ€è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°æ˜¯æ’å®šçš„ï¼‰ä¸­é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’å¯»æ‰¾æœ€å¤§åŒ–å¥–åŠ±çš„è¡Œä¸ºã€‚<br>åœ¨å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“åŸºäº <strong>å½“å‰ç¯å¢ƒ</strong> é‡‡å–æŸäº›actionå¹¶ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œè¿™äº›åŠ¨ä½œå¯èƒ½ä¼šæ”¹å˜ç¯å¢ƒçš„çŠ¶æ€ï¼Œç¯å¢ƒæ ¹æ®äººä¸ºå®šä¹‰çš„å¥–åŠ±å‡½æ•°å‘æ™ºèƒ½ä½“åé¦ˆå¥–åŠ±rã€‚å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥ä¼˜åŒ–åˆ° <strong>è´å°”æ›¼æœ€ä¼˜å…¬å¼</strong> ä¸ºç›®æ ‡ï¼Œå¯»æ‰¾æœ€ä¼˜ç­–ç•¥Ï€âˆ—(a|s)  </p>
<hr>
<p>åŸºæœ¬æ¦‚å¿µï¼š</p>
<ul>
<li>state transition probability: $p(sâ€™|s,a)$</li>
<li>reward function: $r(s,a)$</li>
<li>policy function: $\pi(a|s)$, the only variable while training</li>
<li>state value:<br>$v_{\pi}(s)&#x3D;E[\sum_{t&#x3D;0}^{\infty} \gamma^{t}R_{t+1}|S_t&#x3D;s]$<br>$R_{t+1}$ is the immediate reward following current policy<br>matrix form: $v_{\pi}&#x3D;r_{\pi}+\gamma P_{\pi}v_{\pi}$</li>
<li>action value: $v_{\pi}(s)&#x3D;\sum_a \pi(a|s)q_{\pi}(s)$</li>
<li>Bellman equation: $v_{\pi}(s)&#x3D;\sum_a \pi(a|s)(\sum_r p(r|s,a)*r+\sum_{sâ€™}p(sâ€™|s,a)v_{\pi}(sâ€™))$<br>matrix form: $v_{\pi}&#x3D;r_{\pi}+\gamma P_{\pi}v_{\pi}$</li>
<li>Bellman Optimal Equation: for any policy $\pi$ and state s, $v_{\pi*}(s)\geq v_{\pi}(s)$<br>matrix form: $v&#x3D;f(v)&#x3D;{max}<em>{\pi}$ $(r</em>{\pi}+\gamma v)$</li>
</ul>
<hr>
<p><a target="_blank" rel="noopener" href="https://github.com/HenryLiuOZ/Reinforcement-Learning-Notes">my RL notes</a></p>
<h2 id="Chapter2ï¼šå†…åœ¨åŠ¨æœºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆIMRLï¼‰"><a href="#Chapter2ï¼šå†…åœ¨åŠ¨æœºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆIMRLï¼‰" class="headerlink" title="Chapter2ï¼šå†…åœ¨åŠ¨æœºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆIMRLï¼‰"></a>Chapter2ï¼šå†…åœ¨åŠ¨æœºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆIMRLï¼‰</h2><p><a target="_blank" rel="noopener" href="https://github.com/yuqingd/ellm?tab=readme-ov-file">Guiding Pretraining in Reinforcement Learning with Large Language Models</a></p>
<p>å®é™…é—®é¢˜ï¼šâ€When reward functions are sparse, agents often need to carry out a long, specific sequence of actions to achieve target tasks. As action spaces or target behaviors grow more complex, the space of alternative action sequences agents can explore grows combnatorially. In such scenarios, undirected exploration that randomly perturbs actions or policy parameters has littlechance of succeedingâ€ ä»£ç†åœ¨å¥–åŠ±ç¨€ç–ç¯å¢ƒä¸­è®­ç»ƒå›°éš¾ï¼Œäººä¸ºè®¾è®¡é¢‘ç¹ç²¾ç»†çš„å¥–åŠ±è´¹æ—¶è´¹åŠ›</p>
<p>åˆæ­¥è§£å†³-IMRLï¼šâ€Many distinct action sequences can lead to similar outcomes (Baranes &amp; Oudeyer, 2013)â€”for example, most action sequences cause a humanoid agent to fall, while very few make it walk. Building on this observation, intrinsically motivated RL algorithms (IM-RL) choose to explore outcomes rather than actionsâ€ IMRLåŸºäºæ–°é¢–çš„è¡Œä¸ºç»“æœä¸ºä»£ç†æä¾›å†…åœ¨å¥–åŠ±ä»è€Œä¿ƒä½¿ä»£ç†åšå‡ºä¸åŒçš„æ¢ç´¢ã€‚è€Œä»…ä»…æ˜¯æ¢ç´¢æ›´æ–°å¥‡çš„çŠ¶æ€å¯èƒ½ä¼šä¿ƒä½¿ä»£ç†æ²‰æµ¸åœ¨ä¸æœ€ç»ˆç›®æ ‡æ— å…³çš„è¡Œä¸ºä¸­ã€‚Competence-based Intrinsic MotivationåŸºäºè¯„ä¼°ä»£ç† <em>æŒæ¡æ–°çš„æŠ€èƒ½</em> ç»™å‡ºå¥–åŠ±<br>â€œThe approach we introduce in this paper, ELLM, may be interpreted as a CB-IM algorithm that seeks to explore the space of possible and plausibly useful skills informed by human prior knowledge.â€</p>
<h3 id="CB-IM"><a href="#CB-IM" class="headerlink" title="CB-IM"></a>CB-IM</h3><p>â€œA CB-IM agent is expected to perform well with respect to the original R when the intrinsic reward <strong>Rint(o,a,oâ€™|g)</strong> is both easier to optimize and well aligned with R, such that behaviors maximizing Rint also maximize R.â€ è¿™ä¸ªRintå’Œgoaléœ€è¦æ‰‹åŠ¨è®¾è®¡ â€œMost CB-IM algorithms hand-define the reward functions Rint and the support of the goal distribution in alignment with the original task R, but use various intrinsic motivations to guide goal sampling (1): e.g.novelty, learning progress, intermediate difficultyâ€ è€ŒELLMä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆç›®æ ‡å¹¶è¯„ä¼°ä»£ç†è¡Œä¸º</p>
<h3 id="ELLM"><a href="#ELLM" class="headerlink" title="ELLM"></a>ELLM</h3><p><img src="/images/MARL/Rint_define.png" alt="Rint define"><br><img src="/images/MARL/ELLMprocess.png" alt="ELLMprocess"><br>â€œWe consider two forms of agent training: (1) a goal conditioned setting where the agent is given a sentence embedding of the list of suggested goals, Ï€(a | o,E(g1:k)), and (2) a goal-free setting where the agent does not have access to the suggested goals, Ï€(a | o). While Rint remains the same in either case, training a goal-conditioned agent introduces both challenges and benefits: <strong>it can take time for the agent to learn the meaning of the different goals and connect it to the reward</strong>, but having a language-goal conditioned policy can be moreamenable to downstream tasks than an agent just trained on an exploration reward.â€</p>
<h3 id="è®ºæ–‡æ¯”è¾ƒ-Eureka"><a href="#è®ºæ–‡æ¯”è¾ƒ-Eureka" class="headerlink" title="è®ºæ–‡æ¯”è¾ƒ-Eureka"></a>è®ºæ–‡æ¯”è¾ƒ-Eureka</h3><p>éƒ½æ˜¯è®¾è®¡å¥–åŠ±å‡½æ•°ä»¥è§£å†³ç¯å¢ƒå¥–åŠ±ç¨€ç–é—®é¢˜ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/eureka-research/Eureka?tab=readme-ov-file">Eureka</a>ä¸ELLMæœ‰ä½•ä¸åŒï¼Ÿ<br><img src="/images/Multi-Agents/Eureka1.png" alt="baseline"><br>ç›¸æ¯”ä¹‹ä¸‹ï¼ŒEurekaæ›´åŠ â€œé»‘ç›’â€ã€‚ELLMæ˜¯å¸¦æœ‰é€»è¾‘æ€§çš„(å¦‚æœä½ æ‰¿è®¤LLMæœ‰é€»è¾‘çš„è¯)ï¼Œå®ƒæ ¹æ®æ¸¸æˆè§„åˆ™ä¸ºä»£ç†ç»™å‡ºå»ºè®®ï¼Œå¹¶é¼“åŠ±ä»£ç†è¾¾æˆè¿™äº›ç›®æ ‡ï¼›è€ŒEurekaæ˜¯çº¯ç²¹çš„å¥–åŠ±è®¾è®¡ï¼Œå®ƒå…ˆç”Ÿæˆä¸€æ‰¹å¥–åŠ±å‡½æ•°ï¼Œå†åœ¨ç¯å¢ƒä¸­è®­ç»ƒä»£ç†ï¼ŒæŒ‘é€‰é‚£äº›èƒ½è®©ä»£ç†è®­ç»ƒæ•ˆæœæ›´å¥½çš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶ä¸æ–­è¿­ä»£</p>
<h3 id="ELLM-recurrence"><a href="#ELLM-recurrence" class="headerlink" title="ELLM recurrence"></a>ELLM recurrence</h3><p><a target="_blank" rel="noopener" href="https://gitmind.com/app/docs/mnobpjvz">my code mind map for ellm</a><br>å¯æƒœçš„æ˜¯ï¼Œellmå¹¶æ²¡æœ‰åšä»»ä½•çš„å¯è§†åŒ–ï¼Œæµ‹è¯•å‘˜å”¯ä¸€èƒ½çœ‹åˆ°çš„å°±æ˜¯è®­ç»ƒè¿›ç¨‹çš„tensorboardæˆ–è€…æœ€ç»ˆç”Ÿæˆçš„stats.jsonlæ–‡ä»¶<br>ellmä¸»è¦ä¼ é€’ç»™agentçš„observationæ˜¯æ¸¸æˆç¯å¢ƒçš„textæè¿°+å¯¹text_obsç”¨sentence-transformer tokenizeåŒ–</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_obs</span>(<span class="params">self, obs_dict</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Takes in obs dict and returns a dict where all strings are tokenized.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> <span class="variable language_">self</span>.use_sbert <span class="keyword">and</span> <span class="built_in">isinstance</span>(obs_dict[<span class="string">&#x27;inv_status&#x27;</span>], <span class="built_in">dict</span>):</span><br><span class="line">      inv_status = <span class="string">&quot;&quot;</span></span><br><span class="line">      <span class="keyword">for</span> k, v <span class="keyword">in</span> obs_dict[<span class="string">&#x27;inv_status&#x27;</span>].items():</span><br><span class="line">          <span class="keyword">if</span> v != <span class="string">&#x27;.&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;null&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> v:</span><br><span class="line">              inv_status += v + <span class="string">&quot; &quot;</span></span><br><span class="line">      obs_dict[<span class="string">&#x27;text_obs&#x27;</span>] = obs_dict[<span class="string">&#x27;text_obs&#x27;</span>] + <span class="string">&quot; &quot;</span> + inv_status</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_obs</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a dictionary of text observations&quot;&quot;&quot;</span></span><br><span class="line">    inv, status = <span class="variable language_">self</span>._inventory_to_text()</span><br><span class="line">    obs = <span class="variable language_">self</span>._text_view.local_sentence_view(<span class="variable language_">self</span>.player)</span><br><span class="line">    <span class="keyword">return</span> obs.lower(), &#123;<span class="string">&#x27;inv&#x27;</span> : inv.lower(), <span class="string">&#x27;status&#x27;</span>: status.lower()&#125;</span><br></pre></td></tr></table></figure>

<p>tokenize_obs()ä¸»è¦å°†å½“å‰çš„text_obs()ä¸€ä¸²å­—ç¬¦ä¸²ä½¿ç”¨sentence-transformerè½¬æ¢ä¸ºä¸€ä¸²å€¼ï¼ˆæ–¹ä¾¿ç¥ç»ç½‘ç»œå­¦ä¹ ï¼‰</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EmbeddingView</span>(<span class="title class_ inherited__">SemanticView</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, world, obj_types, grid</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__(world, obj_types)</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._grid = np.array(grid)</span><br><span class="line">    <span class="variable language_">self</span>._offset = <span class="variable language_">self</span>._grid // <span class="number">2</span></span><br><span class="line">    <span class="variable language_">self</span>._area = np.array(<span class="variable language_">self</span>._world.area)</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._names = &#123;value : key <span class="keyword">for</span> (key, value) <span class="keyword">in</span> <span class="variable language_">self</span>._mat_ids.items()&#125;</span><br><span class="line">    <span class="variable language_">self</span>._obj_names = &#123;value : key.__name__ <span class="keyword">for</span> (key, value) <span class="keyword">in</span> <span class="variable language_">self</span>._obj_ids.items()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._names.update(<span class="variable language_">self</span>._obj_names)</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_view</span>(<span class="params">self, player_pos</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return local view of semantic array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    center = np.array(player_pos)</span><br><span class="line">    canvas = <span class="variable language_">self</span>._world._mat_map.copy()</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> <span class="variable language_">self</span>._world.objects:</span><br><span class="line">      canvas[<span class="built_in">tuple</span>(obj.pos)] = <span class="variable language_">self</span>._obj_ids[<span class="built_in">type</span>(obj)]</span><br><span class="line">    local_canvas = np.zeros(<span class="variable language_">self</span>._grid)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>._grid[<span class="number">0</span>]):</span><br><span class="line">      <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>._grid[<span class="number">1</span>]):</span><br><span class="line">        pos = center + np.array([x, y]) - <span class="variable language_">self</span>._offset</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> _inside((<span class="number">0</span>, <span class="number">0</span>), pos, <span class="variable language_">self</span>._area):</span><br><span class="line">          <span class="keyword">continue</span> </span><br><span class="line">        local_canvas[x,y] = canvas[<span class="built_in">tuple</span>(pos)]</span><br><span class="line">    <span class="keyword">return</span> local_canvas</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_token_view</span>(<span class="params">self, player</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      Return a token representation of the observations.</span></span><br><span class="line"><span class="string">      [[&quot;grass&quot;, &quot;grass&quot;, &quot;tree&quot;],</span></span><br><span class="line"><span class="string">       [&quot;grass, &quot;player&quot;, &quot;grass&quot;],</span></span><br><span class="line"><span class="string">       ...] </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    local_canvas = <span class="variable language_">self</span>.local_view(player.pos)</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Right now we replace each index with the corresponding string. </span></span><br><span class="line">    <span class="comment"># In the future, we should place in word embedding directly.</span></span><br><span class="line">    name_canvas = []</span><br><span class="line">    <span class="keyword">for</span> x, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(local_canvas): </span><br><span class="line">      name_canvas.append([])</span><br><span class="line">      <span class="keyword">for</span> y, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(row):</span><br><span class="line">        name = <span class="variable language_">self</span>._names[idx]</span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          name = <span class="string">&#x27;Null&#x27;</span></span><br><span class="line">        name_canvas[x].append(name)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> name_canvas</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_sentence_view</span>(<span class="params">self, player</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      Return a sentence representation of the observations.</span></span><br><span class="line"><span class="string">      &quot;You see cows, trees, and grass&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    local_canvas = <span class="variable language_">self</span>.local_view(player.pos)</span><br><span class="line">    sentence_obs = <span class="string">&quot;You see &quot;</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>è€Œå½“å‰ç¯å¢ƒçš„text_obsç”¨äº†æ¯ä¸ªgameobjectçš„ç¼–å·æ¥ä»£æ›¿å®ƒæœ¬èº«åœ¨æ¸¸æˆä¸­çš„åƒç´ ç‚¹ï¼Œæ‰€ä»¥å–å‡ºæ¥ä¹Ÿæ˜¯ä¸€ä¸ªæ•´æ•°çŸ©é˜µï¼Œç®—æ˜¯ä¸€ä¸ªå–å·§çš„æ–¹æ³•ï¼Œä½†å¯æƒœè¿™ä¸ªæ¸¸æˆå°±å¤±å»äº†å¯è§†åŒ–ç•Œé¢äº†</p>
<p>å›åˆ°æˆ‘æœ€å¼€å§‹çš„ç ”ç©¶æ€è·¯ï¼šå½“å‰ellmæ˜¯é€šè¿‡LMä¸ºä»£ç†æä¾›å»ºè®®ï¼Œå¹¶å¥–åŠ±ä»£ç†è¾¾æˆè¿™äº›å»ºè®®ï¼Œæˆ‘çš„æƒ³æ³•æ˜¯ä¸ºä»€ä¹ˆä¸ç›´æ¥è®©LMç”Ÿæˆä¸€æ¡è·¯å¾„ï¼Œç›´æ¥ <em>æ“çºµä»£ç†å»æ‰§è¡Œè¿™æ¡åŠ¨ä½œè·¯å¾„</em> ï¼Œè¿™æ ·ä¹Ÿå…å»äº†ä»£ç†éœ€è¦å­¦ä¹ åšå‡ºåŠ¨ä½œå’Œå¯¹åº”å»ºè®®ä¹‹é—´å…³ç³»çš„è®­ç»ƒæ—¶é—´<br><strong>ellmå’Œè½¨è¿¹ç”Ÿæˆçš„åŒºåˆ«å®é™…å¾ˆå°</strong>ï¼šæˆ‘å½“æ—¶ä»¥ä¸ºellmç”Ÿæˆçš„å»ºè®®ä¾æ—§æ˜¯éœ€è¦å¤šä¸ªæ—¶é—´æ­¥ä»¥è¾¾æˆçš„å»ºè®®ï¼Œæ¯”å¦‚å»ºè®®ä»£ç†ç°åœ¨åšä¸€å¼ æ¡Œå­ï¼Œå¯èƒ½éœ€è¦å…ˆå»ç æ ‘ï¼Œå†åˆæˆï¼›äº‹å®ä¸Šellmé‡‡ç”¨çš„crafterç¯å¢ƒæ˜¯ä¸€ä¸ªè¿‡äºç®€å•çš„æ¸¸æˆç¯å¢ƒï¼Œå»ºè®®æ˜¯åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ç”Ÿæˆï¼Œå¹¶ä¸”ä¿è¯æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½èƒ½è¾¾æˆä¸€ä¸ªå»ºè®®ï¼ˆå»ºè®®å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªactionï¼Œè¿™å’Œç”Ÿæˆä¸€ä¸ªtrajectoryä¹Ÿæ²¡ä»€ä¹ˆåŒºåˆ«äº†ï¼‰<br><strong>æˆ‘å½“å‰çš„ç ”ç©¶æ€è·¯</strong>ï¼šå°†ellmçš„æ–¹æ³•è¿ç”¨åˆ°æˆ‘çš„FRCThreeç¯å¢ƒä¸­ï¼Œæˆ‘å¯ä»¥åƒellmä¸€æ ·ï¼Œå…ˆç”Ÿæˆæ¯ä¸€æ­¥çš„åŠ¨ä½œå»ºè®®ï¼Œå†è®¡ç®—å½“å‰åŠ¨ä½œå’Œä¸Šä¸€æ¬¡å»ºè®®ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼›ä¹Ÿå¯ä»¥æ‰‹åŠ¨å®ç°ä¸€äº›å‡½æ•°ï¼ˆå¦‚move_a_step_toï¼‰å¹¶ä¸”è®©LMåœ¨æ¯ä¸€æ­¥é€‰æ‹©è¿™äº›å‡½æ•°å¹¶è°ƒç”¨ï¼ˆ<em>æ“çºµä»£ç†å»æ‰§è¡Œè¿™æ¡åŠ¨ä½œè·¯å¾„</em>ï¼‰ï¼Œæœ‰æ²¡æœ‰è§‰å¾—è¿™æœ‰ç‚¹åƒå®‹å­¦é•¿ä¸Šä¸ªæ˜ŸæœŸçš„æ±‡æŠ¥ï¼Ÿ<br>æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘è®¤ä¸ºå½“å‰çš„ç®—æ³•ä¸è¶³ä»¥è§£å†³3v3çš„å¤šæ™ºèƒ½ä½“é—®é¢˜ï¼ˆæˆ‘ä»¬èƒ½ä¿è¯LMåŒæ—¶ç»™ä¸‰ä¸ªæœºå™¨äººæä¾›å»ºè®®ä¸ä¼šé€ æˆå†²çªå—ï¼Œæ¯”å¦‚ï¼ŒåŒæ—¶å»ºè®®ä»–ä»¬ä¸‰ä¸ªå»æ‹¿ä¸€ä¸ªNOTEï¼Ÿï¼‰ï¼Œæˆ‘è¿‘æœŸå†çœ‹ä¸€äº›å¤šæ™ºèƒ½ä½“ç®—æ³•çš„æ–‡ç« ï¼Œæ•´ç†ä¸€ä¸‹è‡ªå·±çš„æ€è·¯ï¼Œäº‰åœ¨1æœˆä»½å‰ç¡®å®šè‡ªå·±æœ€ç»ˆçš„ç®—æ³•ç»“æ„</p>
<h2 id="Chapter3ï¼šå¤šæ™ºèƒ½ä½“articleæ‚è°ˆ"><a href="#Chapter3ï¼šå¤šæ™ºèƒ½ä½“articleæ‚è°ˆ" class="headerlink" title="Chapter3ï¼šå¤šæ™ºèƒ½ä½“articleæ‚è°ˆ"></a>Chapter3ï¼šå¤šæ™ºèƒ½ä½“articleæ‚è°ˆ</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10715990">Dynamic Formation Planning and Control for Robot Soccer Game with Multi-Agent Reinforcement Learning and Behavioral Model</a><br>æå‡ºäº†ä¸€ä¸ªéå¯¹ç§°å¯¹æŠ—ç¯å¢ƒ2(è¿›æ”»æ–¹)vs3(é˜²å®ˆæ–¹)ï¼Œé˜²å®ˆæ–¹é‡‡ç”¨PPOè®­ç»ƒï¼Œè€Œè¿›æ”»æ–¹ä½¿ç”¨äº†ä¸€ä¸ªç±»ä¼¼çŠ¶æ€æœºçš„behavior model<br><img src="/images/MARL/soccer2v3.png" alt="soccer2v3"><br><img src="/images/MARL/behavior_model.png" alt="behavior_model"><br>æ§åˆ¶æœºå™¨äººè¿½é€å°çƒï¼šu &#x3D; qdâ€™- Kp(q-qd)ï¼Œqdä¸ºå°çƒä½ç½®ï¼Œqä¸ºæœºå™¨äººä½ç½®ï¼Œuä¸ºç”µæœºå†²å‡»è¾“å…¥<br>é˜µå‹æ§åˆ¶ï¼šu &#x3D; qdâ€™ - Kp(q-qd) - Î±Î”Ïˆï¼Œå…¶ä¸­qdä¸ºè§„åˆ’å™¨ç”Ÿæˆçš„ç›®æ ‡ä½ç½®ï¼ŒÎ”Ïˆ &#x3D; {Ïˆ1-Ïˆn,Ïˆ2-Ïˆ1,â€¦,Ïˆn-Ïˆn-1}ï¼ŒÏˆ1 &#x3D; Î”q1-Î”q2ï¼ŒÏˆn &#x3D; Î”qn-Î”q1ï¼ŒÎ”qi&#x3D;qi-qdi<br>æˆ‘è§‰å¾—ç»“æœä¸€èˆ¬ï¼Œé˜µå‹æ§åˆ¶çš„å»ºæ¨¡çœ‹ä¸Šå»æœ‰ç”¨ä½†æ˜¯æ²¡æœ‰æºç </p>
<p><a target="_blank" rel="noopener" href="https://collaborative-mapush.github.io/">Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing</a>å¡å†…åŸºæ¢…éš†å¤§å­¦å’ŒGoogleDeepMindåˆä½œçš„æ–‡ç« <br>ç®—æ³•æå‡ºäº†ä¸€ä¸ªåˆ†å±‚å¼MARLæ¡†æ¶ï¼Œç›®çš„æ˜¯è®©&gt;&#x3D;2ä¸ªå››è¶³æœºå™¨ç‹—æ¨åŠ¨æœ¨ç®±åˆ°è¾¾é•¿è·ç¦»çš„ç›®æ ‡ç‚¹<br><img src="/images/MARL/related_work.png" alt="related_work"><br>é«˜å±‚æ§åˆ¶å™¨ï¼šRRTï¼šç‰©ä½“å½“å‰ä½ç½®ã€ç›®æ ‡ä½ç½®ã€éšœç¢ç‰©ä½ç½®-&gt;å¯è¡Œçš„è½¨è¿¹ğœ + é›†ä¸­çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•åšfinetuneï¼šæœ¨ç®±å½“å‰ä½ç½®ã€æœ¨ç®±ç›®æ ‡ä½ç½®ã€ğœã€æ‰€æœ‰æœºå™¨ç‹—çš„å½“å‰çŠ¶æ€-&gt;æœ¨ç®±çš„å­ç›®æ ‡ä½ç½®ahï¼ŒRh&#x3D;æœ¨ç®±è¾¾åˆ°ç›®æ ‡ç‚¹å¥–åŠ±+å­ç›®æ ‡æ¥è¿‘è½¨è¿¹ğœçš„å¥–åŠ±+å¯¹è·ç¦»éšœç¢ç‰©è¾ƒè¿‘çš„æƒ©ç½šä»¥åŠå¯¹æœºå™¨äººè·Œå€’ã€ç¢°æ’ã€ç‰©ä½“å€¾æ–œå’Œè¶…æ—¶ç­‰å¼‚å¸¸æƒ…å†µçš„ä¸¥å‰æƒ©ç½š<br><img src="/images/MARL/high_level_RL.png" alt="high_level_RL"><br>ä¸­å±‚æ§åˆ¶å™¨ï¼šå»ä¸­å¿ƒåŒ–MAPPOå¼ºåŒ–å­¦ä¹ ï¼šæœºå™¨ç‹—è‡ªèº«è§‚æµ‹o+å­ç›®æ ‡ah-&gt;amæœºå™¨ç‹—çš„ç›®æ ‡é€Ÿåº¦å‘é‡ï¼Œæœ‰ä¸€ä¸ªæ¯”è¾ƒç‰¹åˆ«çš„OCB reward&#x3D;<strong>v</strong>i â‹… <strong>v</strong>targetï¼Œæœºå™¨ç‹—åˆ°æœ¨ç®±æœ€è¿‘ç‚¹çš„å‘é‡ã€æœ¨ç®±åˆ°å…¶å­ç›®æ ‡çš„å‘é‡<br><img src="/images/MARL/OCB_reward.png" alt="OCB_reward"><br>ä½å±‚æ§åˆ¶å™¨ï¼šam-&gt;alæœºå™¨ç‹—ç”µæœºæ§åˆ¶ï¼Œç›´æ¥ç”¨äº†Unitreeå†…ç½®çš„<br>è¿™ä¸ªé«˜å±‚æ§åˆ¶å™¨çš„æ¶æ„çœ‹ä¸Šå»æœ‰ç”¨ï¼Œæˆ–è®¸æˆ‘ä¹Ÿå¯ä»¥ç”¨RLå¯¹LLMç”Ÿæˆçš„å­ç›®æ ‡åšfinetune?é€šè¿‡RLå¯¹LLMç”Ÿæˆçš„å»ºè®®ä½œé€‰æ‹©ä¸ä¼˜åŒ–</p>
<p><a target="_blank" rel="noopener" href="https://ml-jku.github.io/rudder/">RUDDER</a><br>å°†LSTMåº”ç”¨äºreward-delayed RLçš„å¼€åˆ›æ€§æ–‡ç« (2018)ï¼Œè§£å†³çš„é—®é¢˜æ˜¯å…·æœ‰å»¶è¿Ÿå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒåŠ¨ä½œä»·å€¼å‡½æ•°å…·æœ‰é«˜åå·®ï¼Œä»£ç†è®­ç»ƒç¼“æ…¢ï¼Œè§£å†³æ–¹å¼æ˜¯å¥–åŠ±é‡åˆ†é…ã€‚<br>RUDDER ä½¿ç”¨ LSTM æ ¹æ®å½“å‰çŠ¶æ€ä»¥åŠå…ˆå‰çŠ¶æ€å’Œæ“ä½œçš„é¡ºåºé¢„æµ‹æ¯ä¸ªæ—¶é—´æ­¥çš„æœ€ç»ˆç´¯ç§¯å¥–åŠ±ï¼ˆå›æŠ¥ï¼‰ï¼Œä»¥ç¡®å®šå“ªäº›äº‹ä»¶å¯¹æœ€ç»ˆå¥–åŠ±çš„è´¡çŒ®æœ€å¤§ã€‚ä¸€æ—¦ LSTM è¯†åˆ«å‡ºå¯¼è‡´æœ€ç»ˆå¥–åŠ±çš„é‡è¦äº‹ä»¶ï¼ŒRUDDER å°±ä¼šå°†å¤–éƒ¨å¥–åŠ±é‡æ–°åˆ†é…ç»™é‚£äº›å…³é”®æ—¶é—´æ­¥ã€‚æ–‡ç« æœ‰ä¸¥è°¨çš„è¯æ˜å¥–åŠ±é‡åˆ†é…ä¸æ”¹å˜æ¸¸æˆçš„æ€»å›æŠ¥&#x2F;æœ€ä¼˜ç­–ç•¥<br>é—®é¢˜æ˜¯è®­ç»ƒä¸€ä¸ªLSTMæœ¬èº«ä¹Ÿéœ€è¦æ—¶é—´ï¼Œæˆ–è®¸<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.09999">reward centering</a>å·²ç»æ˜¯ä¸€ä¸ªè¶³å¤Ÿå¥½çš„æƒ³æ³•<br>ML-Agentsä¹Ÿå¯ä»¥ä½¿ç”¨LSTMï¼Œä½†å¹¶ä¸æ˜¯RUDDERç®—æ³•ï¼Œè€Œæ˜¯DRQNï¼š<a target="_blank" rel="noopener" href="https://github.com/miyamotok0105/unity-ml-agents/blob/master/docs/Feature-Memory.md">ä»£ç†å°†èƒ½å¤Ÿå­˜å‚¨æµ®ç‚¹å‘é‡ï¼Œä»¥ä¾¿åœ¨ä¸‹æ¬¡éœ€è¦åšå‡ºå†³ç­–æ—¶ä½¿ç”¨</a></p>
<h3 id="ç»å…¸å¤šæ™ºèƒ½ä½“ç®—æ³•"><a href="#ç»å…¸å¤šæ™ºèƒ½ä½“ç®—æ³•" class="headerlink" title="- ç»å…¸å¤šæ™ºèƒ½ä½“ç®—æ³•"></a>- ç»å…¸å¤šæ™ºèƒ½ä½“ç®—æ³•</h3><p>ä¸»è¦ä»‹ç»é›†ä¸­è®­ç»ƒï¼Œåˆ†æ•£æ‰§è¡Œçš„MARLç®—æ³•</p>
<p>å€¼åˆ†è§£ç±»å‹ï¼š<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.11485">QMIX</a><br>åŸºäºVDNç®—æ³•æ”¹è¿›ï¼Œä¸»è¦æ€æƒ³æ˜¯ä¼˜åŒ–ä¸€ä¸ªQtotç½‘ç»œ(VDNçš„Qtotæ˜¯Qiçš„çº¿æ€§åŠ å’Œï¼Œå¯æƒ³è€ŒçŸ¥æœ‰å¾ˆå¤šé—®é¢˜)ï¼ŒQtotè¾“å…¥å½“å‰ç¯å¢ƒè§‚æµ‹Ï„,æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œai(ä»¥åŠæ¯ä¸ªæ™ºèƒ½ä½“Qiç½‘ç»œçš„è¾“å‡º)ï¼Œè¾“å‡ºå½“å‰çŠ¶æ€sä¸‹æ™ºèƒ½ä½“åšå‡ºè”åˆåŠ¨ä½œaçš„é¢„æœŸå›æŠ¥ï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯è¶‹è¿‘æœ€ä¼˜ç­–ç•¥ä¸‹(Ï„,a)çš„çœŸå®åŠ¨ä½œä»·å€¼ï¼Œå…¶ä¸­Ï„ is a joint actionobservation historyï¼Œç”¨RNNè§£å†³Dec-POMDPçš„ç¯å¢ƒéƒ¨åˆ†å¯è§‚æµ‹é—®é¢˜<br><img src="/images/MARL/QMIX.png" alt="QMIX"><br>æ–‡ç« å‡è®¾ç»™å®šç¯å¢ƒè§‚æµ‹Ï„ï¼ŒQtot(Ï„,a)å–æœ€å¤§å€¼æ—¶ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“Qi(Ï„i,ai)ä¸€å®šå–ç»™å®šè§‚æµ‹Ï„iä¸‹çš„æœ€å¤§å€¼ï¼Œæˆ–è€…è¡¨ç¤ºä¸º <strong>âˆ‚Qtot&#x2F;âˆ‚Qi&gt;&#x3D;0, âˆ€i&lt;&#x3D;N</strong><br><img src="/images/MARL/Qtot.png" alt="Qtot"><br>ç®—æ³•å…ˆé€šè¿‡group rewardä¼˜åŒ–Qtotç½‘ç»œï¼Œå†å¯¹Qtotæ±‚æ¢¯åº¦åå‘ä¼ æ’­å°±å¯ä»¥ä¼˜åŒ–æ¯ä¸ªæ™ºèƒ½ä½“çš„Qç½‘ç»œ<br>ç®—æ³•å‡è®¾ç¯å¢ƒä¸­åªæœ‰ä¸€ä¸ªå”¯ä¸€çš„å›¢é˜Ÿå¥–åŠ±ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“æ²¡æœ‰è‡ªå·±ç‹¬ç«‹çš„å¥–åŠ±å‡½æ•°</p>
<p>Actor-Crticç±»å‹ï¼š<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02275">MADDPG</a>ï¼Œ<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53811876">çŸ¥ä¹</a>è¿™ç¯‡è®²çš„å·²ç»å¾ˆç²¾ç‚¼äº†<br><img src="/images/MARL/MADDPG.png" alt="MADDPG"><br>MADDPGæ¶æ„ä¸­ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½æœ‰è‡ªå·±çš„ä¸­å¿ƒåŒ–ä»·å€¼ç½‘ç»œå’Œå»ä¸­å¿ƒåŒ–ç­–ç•¥ç½‘ç»œï¼Œå…è®¸æ¯ä¸ªæ™ºèƒ½ä½“æœ‰ç‹¬ç«‹çš„å¥–åŠ±å‡½æ•°ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œç©ºé—´ï¼Œè§‚æµ‹ç©ºé—´ä¹Ÿå¯ä»¥ä¸ä¸€æ ·</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.08926">COMA</a> é€šè¿‡åäº‹å®æ¨ç†&#x2F;å·®å¼‚å¥–åŠ±&#x2F;ä¼˜åŠ¿å‡½æ•°å®ç°credit distributionï¼Œä»è€Œæ›´æ–°policy network<br>ä¸MADDPGä¸åŒï¼ŒCOMAå‡è®¾ç¯å¢ƒåªå­˜åœ¨group rewardï¼Œå¹¶åªä½¿ç”¨ä¸€ä¸ªä¸­å¿ƒåŒ–çš„Criticç½‘ç»œè¯„ä¼°æ™ºèƒ½ä½“è”åˆåŠ¨ä½œçš„ä»·å€¼<br><img src="/images/MARL/COMA.png" alt="COMA"><br>ä¼˜åŠ¿å‡½æ•°ä¸ºï¼š<br><img src="/images/MARL/Advantage_function.png" alt="Advantage_function"><br>policy networkæ›´æ–°é€šè¿‡ï¼š<br><img src="/images/MARL/COMA_policy_update.png" alt="COMA_policy_update"></p>
<p><a target="_blank" rel="noopener" href="https://rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf">MA-POCA</a>ï¼šUnityå‘è¡¨çš„å¤šæ™ºèƒ½ä½“ç®—æ³•ï¼Œä¸»è¦åœ¨COMAç®—æ³•çš„åŸºç¡€ä¸ŠåŠ ä¸Šäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚ä½œè€…ç‹¬åˆ°åœ°æ³¨æ„åˆ°äº†ç°æœ‰å¤šæ™ºèƒ½ä½“ç®—æ³•æ— æ³•å¤„ç†å¯å˜å¤šæ™ºèƒ½ä½“æ€»æ•°çš„æƒ…å†µ <em>â€œ To the authorâ€™s knowledge, this is first time the posthumous credit assignment problem has been explicitly mentioned in the literature.â€</em> ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å°†å˜é•¿çš„é›†ä¸­è§‚æµ‹ã€é›†ä¸­åŠ¨ä½œè½¬åŒ–ä¸ºäº†å®šé•¿çš„ç‰¹å¾å‘é‡ï¼Œç”¨ä½œä»·å€¼å‡½æ•°çš„è¾“å…¥ã€‚<br>ç®—æ³•å‡è®¾äº†ä¸€ä¸ªå·²çŸ¥æ€»æ™ºèƒ½ä½“æ€»æ•°ä¸è¶…è¿‡Nï¼Œä½†åœ¨æ¯ä¸€æ—¶é—´æ­¥æ™ºèƒ½ä½“æ•°ç›®ä¸ç¡®å®šçš„ç¯å¢ƒ(æœ‰æ–°æ™ºèƒ½ä½“äº§ç”Ÿæˆ–æ­»äº¡)ï¼Œæ­»äº¡çš„æ™ºèƒ½ä½“è¢«è§†ä¸ºæ— æ³•åšå‡ºåŠ¨ä½œä»¥æ”¹å˜ç¯å¢ƒ<br>ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ié¢å¤–è®­ç»ƒç¼–ç å‡½æ•°gi(oi),fi(oi,ai)ï¼Œä¸”ä¸¤ä¸ªç¼–ç å‡½æ•°è¾“å‡ºçš„ç‰¹å¾å‘é‡å¤§å°ç›¸åŒä¸ºd<br><img src="/images/MARL/POCA_center_critic.png" alt="POCA_center_critic"><br>åäº‹å®ä¼˜åŠ¿å‡½æ•°ï¼š<br><img src="/images/MARL/POCA_advantage.png" alt="POCA_advantage"><br>RSA(gj (oj), fi(oi,ai) i!&#x3D;j )è¿”å›äº†ç»¼åˆäº†ojå’Œ(oi,ai)ä¿¡æ¯çš„ç‰¹å¾å‘é‡ï¼Œå¤§å°ä»ä¸ºdï¼Œä»è€Œå¯ä»¥å¤„ç†ä»»æ„æ•°é‡çš„ä»£ç†è”åˆåŠ¨ä½œ</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02511">LEMAE</a>å’Œ<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.03997">You Only LLM Once for MARL</a>çš„æƒ³æ³•æ¯”è¾ƒæ¥è¿‘ï¼Œé€šè¿‡å¯¹LLM chain-of-thought å¼çš„æé—®å¼•å¯¼ï¼Œä»¤LLMåˆ†ç¦»å‡ºä»»åŠ¡çš„å…³é”®çŠ¶æ€(å­ä»»åŠ¡)ï¼Œè®¾è®¡å¥–åŠ±å‡½æ•°ç­‰ï¼Œç›®çš„æ˜¯ä¼˜åŒ–äº†ELLMéœ€è¦åå¤è¯¢é—®LLMçš„é—®é¢˜ï¼Œä½†ç›¸åº”çš„éš¾ä»¥å¤„ç†æŒç»­æ€§ä»»åŠ¡</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">USER:</span><br><span class="line">(Task Description)</span><br><span class="line">Two agents are in a 30x30 room with the coordinate of the point in the upper left corner is</span><br><span class="line">(0,0) and the point in the lower right corner is (29,29), separated by a wall with a door.</span><br><span class="line">The door can not be directly open. There is a hidden switch in each room (The switch</span><br><span class="line">coordinates are unknown and differ from the coordinate of the door.). The door will be open</span><br><span class="line">only when one of the switch is occupied by an agent otherwise close. Both agents begin in</span><br><span class="line">the left half of the room(agent0.x&lt;15 and agent1.x&lt;15).</span><br><span class="line">The task is considered successfully completed as long as both agents are located in the right</span><br><span class="line">half-room(agent0.x&gt;15 and agent1.x&gt;15).</span><br><span class="line">(State Form)</span><br><span class="line">In each step we can get current state. The state is a 1-dim list:</span><br><span class="line">(agent0.x, agent0.y, agent1.x, agent1.y,door.open).</span><br><span class="line">(Role Instruction)(Template)</span><br><span class="line">Your role is to give several critical key states in the task...</span><br><span class="line">...</span><br><span class="line">ASSISTANT: (LLM response)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line">Key states: &#123;</span><br><span class="line">init:Initial state where both agents are in the left half of the room.,</span><br><span class="line">key state 1:Agent0 has found the switch and is standing on it, opening the door,</span><br><span class="line">key state 2:Agent1 has moved through the open door to the right half of the room while</span><br><span class="line">agent0 is still on the switch,</span><br><span class="line">key state 3:Agent0 has joined agent1 in the right half of the room</span><br><span class="line">&#125;,</span><br><span class="line">Discriminators: &#123;</span><br><span class="line">key state 1:def iskeystate1(state): return 1 if state[4] == 1 and state[0] &gt;15 else 0,</span><br><span class="line">key state 2:def iskeystate2(state): return 1 if state[4] == 1 and state[2] &gt;15 and state[0]</span><br><span class="line">&lt;15 else 0,</span><br><span class="line">key state 3:def iskeystate3(state): return 1 if state[0] &gt;15 and state[2] &gt;15 else 0</span><br><span class="line">&#125;,</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/images/MARL/co_pass.png" alt="co_pass"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/09/08/Reinforcement-Learning/" data-id="cm45fdq7n0000gsk31qsj4gnc" data-title="Reinforcement Learning" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/09/02/Multi-Agents/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Multi Agents</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/09/08/Reinforcement-Learning/">Reinforcement Learning</a>
          </li>
        
          <li>
            <a href="/2024/09/02/Multi-Agents/">Multi Agents</a>
          </li>
        
          <li>
            <a href="/2024/07/28/ROS2-Learning/">ROS2_Learning</a>
          </li>
        
          <li>
            <a href="/2024/07/28/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>