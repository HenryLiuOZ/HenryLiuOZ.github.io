<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Reinforcement Learning | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="此博客主要介绍强化学习算法，相关论文也会放在里面 Chapter1：强化学习背景与基础知识参考：huggingfaceWindyLab 单智能体强化学习旨在解决一类马尔可夫决策过程 (马尔可夫性：环境状态转移的概率仅依赖于当前状态和动作) 的问题。强化学习的假设是代理能够在一个静态环境 environment with stationary distribution （状态转移概率和奖励函数是恒定">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning">
<meta property="og:url" content="http://example.com/2024/09/08/Reinforcement-Learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="此博客主要介绍强化学习算法，相关论文也会放在里面 Chapter1：强化学习背景与基础知识参考：huggingfaceWindyLab 单智能体强化学习旨在解决一类马尔可夫决策过程 (马尔可夫性：环境状态转移的概率仅依赖于当前状态和动作) 的问题。强化学习的假设是代理能够在一个静态环境 environment with stationary distribution （状态转移概率和奖励函数是恒定">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/MARL/Rint_define.png">
<meta property="og:image" content="http://example.com/images/MARL/ELLMprocess.png">
<meta property="og:image" content="http://example.com/images/Multi-Agents/Eureka1.png">
<meta property="og:image" content="http://example.com/images/MARL/soccer2v3.png">
<meta property="og:image" content="http://example.com/images/MARL/behavior_model.png">
<meta property="og:image" content="http://example.com/images/MARL/related_work.png">
<meta property="og:image" content="http://example.com/images/MARL/high_level_RL.png">
<meta property="og:image" content="http://example.com/images/MARL/OCB_reward.png">
<meta property="og:image" content="http://example.com/images/MARL/QMIX.png">
<meta property="og:image" content="http://example.com/images/MARL/Qtot.png">
<meta property="og:image" content="http://example.com/images/MARL/MADDPG.png">
<meta property="og:image" content="http://example.com/images/MARL/COMA.png">
<meta property="og:image" content="http://example.com/images/MARL/Advantage_function.png">
<meta property="og:image" content="http://example.com/images/MARL/COMA_policy_update.png">
<meta property="og:image" content="http://example.com/images/MARL/POCA_center_critic.png">
<meta property="og:image" content="http://example.com/images/MARL/POCA_advantage.png">
<meta property="og:image" content="http://example.com/images/MARL/co_pass.png">
<meta property="article:published_time" content="2024-09-08T09:05:10.000Z">
<meta property="article:modified_time" content="2025-03-10T06:10:52.095Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/MARL/Rint_define.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/09/08/Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2024-09-08T09:05:10.000Z" itemprop="datePublished">2024-09-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Reinforcement Learning
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>此博客主要介绍强化学习算法，相关论文也会放在里面</p>
<h2 id="Chapter1：强化学习背景与基础知识"><a href="#Chapter1：强化学习背景与基础知识" class="headerlink" title="Chapter1：强化学习背景与基础知识"></a>Chapter1：强化学习背景与基础知识</h2><p>参考：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit1/what-is-rl">huggingface</a><br><a target="_blank" rel="noopener" href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning">WindyLab</a></p>
<p>单智能体强化学习旨在解决一类马尔可夫决策过程 (马尔可夫性：环境状态转移的概率仅依赖于当前状态和动作) 的问题。强化学习的假设是代理能够在一个静态环境 <em>environment with stationary distribution</em> （状态转移概率和奖励函数是恒定的）中通过与环境的交互寻找最大化奖励的行为。<br>在单智能体强化学习中，智能体基于 <strong>当前环境</strong> 采取某些action并与环境进行交互，这些动作可能会改变环境的状态，环境根据人为定义的奖励函数向智能体反馈奖励r。强化学习算法以优化到 <strong>贝尔曼最优公式</strong> 为目标，寻找最优策略π∗(a|s)  </p>
<hr>
<p>基本概念：</p>
<ul>
<li>state transition probability: $p(s’|s,a)$</li>
<li>reward function: $r(s,a)$</li>
<li>policy function: $\pi(a|s)$, the only variable while training</li>
<li>state value:<br>$v_{\pi}(s)&#x3D;E[\sum_{t&#x3D;0}^{\infty} \gamma^{t}R_{t+1}|S_t&#x3D;s]$<br>$R_{t+1}$ is the immediate reward following current policy<br>matrix form: $v_{\pi}&#x3D;r_{\pi}+\gamma P_{\pi}v_{\pi}$</li>
<li>action value: $v_{\pi}(s)&#x3D;\sum_a \pi(a|s)q_{\pi}(s)$</li>
<li>Bellman equation: $v_{\pi}(s)&#x3D;\sum_a \pi(a|s)(\sum_r p(r|s,a)*r+\sum_{s’}p(s’|s,a)v_{\pi}(s’))$<br>matrix form: $v_{\pi}&#x3D;r_{\pi}+\gamma P_{\pi}v_{\pi}$</li>
<li>Bellman Optimal Equation: for any policy $\pi$ and state s, $v_{\pi*}(s)\geq v_{\pi}(s)$<br>matrix form: $v&#x3D;f(v)&#x3D;{max}<em>{\pi}$ $(r</em>{\pi}+\gamma v)$</li>
</ul>
<hr>
<p><a target="_blank" rel="noopener" href="https://github.com/HenryLiuOZ/Reinforcement-Learning-Notes">my RL notes</a></p>
<h2 id="Chapter2：内在动机强化学习算法（IMRL）"><a href="#Chapter2：内在动机强化学习算法（IMRL）" class="headerlink" title="Chapter2：内在动机强化学习算法（IMRL）"></a>Chapter2：内在动机强化学习算法（IMRL）</h2><p><a target="_blank" rel="noopener" href="https://github.com/yuqingd/ellm?tab=readme-ov-file">Guiding Pretraining in Reinforcement Learning with Large Language Models</a></p>
<p>实际问题：”When reward functions are sparse, agents often need to carry out a long, specific sequence of actions to achieve target tasks. As action spaces or target behaviors grow more complex, the space of alternative action sequences agents can explore grows combnatorially. In such scenarios, undirected exploration that randomly perturbs actions or policy parameters has littlechance of succeeding” 代理在奖励稀疏环境中训练困难，人为设计频繁精细的奖励费时费力</p>
<p>初步解决-IMRL：”Many distinct action sequences can lead to similar outcomes (Baranes &amp; Oudeyer, 2013)—for example, most action sequences cause a humanoid agent to fall, while very few make it walk. Building on this observation, intrinsically motivated RL algorithms (IM-RL) choose to explore outcomes rather than actions” IMRL基于新颖的行为结果为代理提供内在奖励从而促使代理做出不同的探索。而仅仅是探索更新奇的状态可能会促使代理沉浸在与最终目标无关的行为中。Competence-based Intrinsic Motivation基于评估代理 <em>掌握新的技能</em> 给出奖励<br>“The approach we introduce in this paper, ELLM, may be interpreted as a CB-IM algorithm that seeks to explore the space of possible and plausibly useful skills informed by human prior knowledge.”</p>
<h3 id="CB-IM"><a href="#CB-IM" class="headerlink" title="CB-IM"></a>CB-IM</h3><p>“A CB-IM agent is expected to perform well with respect to the original R when the intrinsic reward <strong>Rint(o,a,o’|g)</strong> is both easier to optimize and well aligned with R, such that behaviors maximizing Rint also maximize R.” 这个Rint和goal需要手动设计 “Most CB-IM algorithms hand-define the reward functions Rint and the support of the goal distribution in alignment with the original task R, but use various intrinsic motivations to guide goal sampling (1): e.g.novelty, learning progress, intermediate difficulty” 而ELLM使用大语言模型自动生成目标并评估代理行为</p>
<h3 id="ELLM"><a href="#ELLM" class="headerlink" title="ELLM"></a>ELLM</h3><p><img src="/images/MARL/Rint_define.png" alt="Rint define"><br><img src="/images/MARL/ELLMprocess.png" alt="ELLMprocess"><br>“We consider two forms of agent training: (1) a goal conditioned setting where the agent is given a sentence embedding of the list of suggested goals, π(a | o,E(g1:k)), and (2) a goal-free setting where the agent does not have access to the suggested goals, π(a | o). While Rint remains the same in either case, training a goal-conditioned agent introduces both challenges and benefits: <strong>it can take time for the agent to learn the meaning of the different goals and connect it to the reward</strong>, but having a language-goal conditioned policy can be moreamenable to downstream tasks than an agent just trained on an exploration reward.”</p>
<h3 id="论文比较-Eureka"><a href="#论文比较-Eureka" class="headerlink" title="论文比较-Eureka"></a>论文比较-Eureka</h3><p>都是设计奖励函数以解决环境奖励稀疏问题，<a target="_blank" rel="noopener" href="https://github.com/eureka-research/Eureka?tab=readme-ov-file">Eureka</a>与ELLM有何不同？<br><img src="/images/Multi-Agents/Eureka1.png" alt="baseline"><br>相比之下，Eureka更加“黑盒”。ELLM是带有逻辑性的(如果你承认LLM有逻辑的话)，它根据游戏规则为代理给出建议，并鼓励代理达成这些目标；而Eureka是纯粹的奖励设计，它先生成一批奖励函数，再在环境中训练代理，挑选那些能让代理训练效果更好的奖励函数，并不断迭代</p>
<h3 id="ELLM-recurrence"><a href="#ELLM-recurrence" class="headerlink" title="ELLM recurrence"></a>ELLM recurrence</h3><p><a target="_blank" rel="noopener" href="https://gitmind.com/app/docs/mnobpjvz">my code mind map for ellm</a><br>可惜的是，ellm并没有做任何的可视化，测试员唯一能看到的就是训练进程的tensorboard或者最终生成的stats.jsonl文件<br>ellm主要传递给agent的observation是游戏环境的text描述+对text_obs用sentence-transformer tokenize化</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_obs</span>(<span class="params">self, obs_dict</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Takes in obs dict and returns a dict where all strings are tokenized.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> <span class="variable language_">self</span>.use_sbert <span class="keyword">and</span> <span class="built_in">isinstance</span>(obs_dict[<span class="string">&#x27;inv_status&#x27;</span>], <span class="built_in">dict</span>):</span><br><span class="line">      inv_status = <span class="string">&quot;&quot;</span></span><br><span class="line">      <span class="keyword">for</span> k, v <span class="keyword">in</span> obs_dict[<span class="string">&#x27;inv_status&#x27;</span>].items():</span><br><span class="line">          <span class="keyword">if</span> v != <span class="string">&#x27;.&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;null&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> v:</span><br><span class="line">              inv_status += v + <span class="string">&quot; &quot;</span></span><br><span class="line">      obs_dict[<span class="string">&#x27;text_obs&#x27;</span>] = obs_dict[<span class="string">&#x27;text_obs&#x27;</span>] + <span class="string">&quot; &quot;</span> + inv_status</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_obs</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a dictionary of text observations&quot;&quot;&quot;</span></span><br><span class="line">    inv, status = <span class="variable language_">self</span>._inventory_to_text()</span><br><span class="line">    obs = <span class="variable language_">self</span>._text_view.local_sentence_view(<span class="variable language_">self</span>.player)</span><br><span class="line">    <span class="keyword">return</span> obs.lower(), &#123;<span class="string">&#x27;inv&#x27;</span> : inv.lower(), <span class="string">&#x27;status&#x27;</span>: status.lower()&#125;</span><br></pre></td></tr></table></figure>

<p>tokenize_obs()主要将当前的text_obs()一串字符串使用sentence-transformer转换为一串值（方便神经网络学习）</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EmbeddingView</span>(<span class="title class_ inherited__">SemanticView</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, world, obj_types, grid</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__(world, obj_types)</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._grid = np.array(grid)</span><br><span class="line">    <span class="variable language_">self</span>._offset = <span class="variable language_">self</span>._grid // <span class="number">2</span></span><br><span class="line">    <span class="variable language_">self</span>._area = np.array(<span class="variable language_">self</span>._world.area)</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._names = &#123;value : key <span class="keyword">for</span> (key, value) <span class="keyword">in</span> <span class="variable language_">self</span>._mat_ids.items()&#125;</span><br><span class="line">    <span class="variable language_">self</span>._obj_names = &#123;value : key.__name__ <span class="keyword">for</span> (key, value) <span class="keyword">in</span> <span class="variable language_">self</span>._obj_ids.items()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._names.update(<span class="variable language_">self</span>._obj_names)</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_view</span>(<span class="params">self, player_pos</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return local view of semantic array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    center = np.array(player_pos)</span><br><span class="line">    canvas = <span class="variable language_">self</span>._world._mat_map.copy()</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> <span class="variable language_">self</span>._world.objects:</span><br><span class="line">      canvas[<span class="built_in">tuple</span>(obj.pos)] = <span class="variable language_">self</span>._obj_ids[<span class="built_in">type</span>(obj)]</span><br><span class="line">    local_canvas = np.zeros(<span class="variable language_">self</span>._grid)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>._grid[<span class="number">0</span>]):</span><br><span class="line">      <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>._grid[<span class="number">1</span>]):</span><br><span class="line">        pos = center + np.array([x, y]) - <span class="variable language_">self</span>._offset</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> _inside((<span class="number">0</span>, <span class="number">0</span>), pos, <span class="variable language_">self</span>._area):</span><br><span class="line">          <span class="keyword">continue</span> </span><br><span class="line">        local_canvas[x,y] = canvas[<span class="built_in">tuple</span>(pos)]</span><br><span class="line">    <span class="keyword">return</span> local_canvas</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_token_view</span>(<span class="params">self, player</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      Return a token representation of the observations.</span></span><br><span class="line"><span class="string">      [[&quot;grass&quot;, &quot;grass&quot;, &quot;tree&quot;],</span></span><br><span class="line"><span class="string">       [&quot;grass, &quot;player&quot;, &quot;grass&quot;],</span></span><br><span class="line"><span class="string">       ...] </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    local_canvas = <span class="variable language_">self</span>.local_view(player.pos)</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Right now we replace each index with the corresponding string. </span></span><br><span class="line">    <span class="comment"># In the future, we should place in word embedding directly.</span></span><br><span class="line">    name_canvas = []</span><br><span class="line">    <span class="keyword">for</span> x, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(local_canvas): </span><br><span class="line">      name_canvas.append([])</span><br><span class="line">      <span class="keyword">for</span> y, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(row):</span><br><span class="line">        name = <span class="variable language_">self</span>._names[idx]</span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          name = <span class="string">&#x27;Null&#x27;</span></span><br><span class="line">        name_canvas[x].append(name)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> name_canvas</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_sentence_view</span>(<span class="params">self, player</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      Return a sentence representation of the observations.</span></span><br><span class="line"><span class="string">      &quot;You see cows, trees, and grass&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    local_canvas = <span class="variable language_">self</span>.local_view(player.pos)</span><br><span class="line">    sentence_obs = <span class="string">&quot;You see &quot;</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>而当前环境的text_obs用了每个gameobject的编号来代替它本身在游戏中的像素点，所以取出来也是一个整数矩阵，算是一个取巧的方法，但可惜这个游戏就失去了可视化界面了</p>
<p>回到我最开始的研究思路：当前ellm是通过LM为代理提供建议，并奖励代理达成这些建议，我的想法是为什么不直接让LM生成一条路径，直接 <em>操纵代理去执行这条动作路径</em> ，这样也免去了代理需要学习做出动作和对应建议之间关系的训练时间<br><strong>ellm和轨迹生成的区别实际很小</strong>：我当时以为ellm生成的建议依旧是需要多个时间步以达成的建议，比如建议代理现在做一张桌子，可能需要先去砍树，再合成；事实上ellm采用的crafter环境是一个过于简单的游戏环境，建议是在每一个时间步生成，并且保证每一个时间步都能达成一个建议（建议实际上就是一个action，这和生成一个trajectory也没什么区别了）<br><strong>我当前的研究思路</strong>：将ellm的方法运用到我的FRCThree环境中，我可以像ellm一样，先生成每一步的动作建议，再计算当前动作和上一次建议之间的相似度；也可以手动实现一些函数（如move_a_step_to）并且让LM在每一步选择这些函数并调用（<em>操纵代理去执行这条动作路径</em>），有没有觉得这有点像宋学长上个星期的汇报？<br>更进一步，我认为当前的算法不足以解决3v3的多智能体问题（我们能保证LM同时给三个机器人提供建议不会造成冲突吗，比如，同时建议他们三个去拿一个NOTE？），我近期再看一些多智能体算法的文章，整理一下自己的思路，争在1月份前确定自己最终的算法结构</p>
<h2 id="Chapter3：多智能体article杂谈"><a href="#Chapter3：多智能体article杂谈" class="headerlink" title="Chapter3：多智能体article杂谈"></a>Chapter3：多智能体article杂谈</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10715990">Dynamic Formation Planning and Control for Robot Soccer Game with Multi-Agent Reinforcement Learning and Behavioral Model</a><br>提出了一个非对称对抗环境2(进攻方)vs3(防守方)，防守方采用PPO训练，而进攻方使用了一个类似状态机的behavior model<br><img src="/images/MARL/soccer2v3.png" alt="soccer2v3"><br><img src="/images/MARL/behavior_model.png" alt="behavior_model"><br>控制机器人追逐小球：u &#x3D; qd’- Kp(q-qd)，qd为小球位置，q为机器人位置，u为电机冲击输入<br>阵型控制：u &#x3D; qd’ - Kp(q-qd) - αΔψ，其中qd为规划器生成的目标位置，Δψ &#x3D; {ψ1-ψn,ψ2-ψ1,…,ψn-ψn-1}，ψ1 &#x3D; Δq1-Δq2，ψn &#x3D; Δqn-Δq1，Δqi&#x3D;qi-qdi<br>我觉得结果一般，阵型控制的建模看上去有用但是没有源码</p>
<p><a target="_blank" rel="noopener" href="https://collaborative-mapush.github.io/">Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing</a>卡内基梅隆大学和GoogleDeepMind合作的文章<br>算法提出了一个分层式MARL框架，目的是让&gt;&#x3D;2个四足机器狗推动木箱到达长距离的目标点<br><img src="/images/MARL/related_work.png" alt="related_work"><br>高层控制器：RRT：物体当前位置、目标位置、障碍物位置-&gt;可行的轨迹𝜏 + 集中的强化学习算法做finetune：木箱当前位置、木箱目标位置、𝜏、所有机器狗的当前状态-&gt;木箱的子目标位置ah，Rh&#x3D;木箱达到目标点奖励+子目标接近轨迹𝜏的奖励+对距离障碍物较近的惩罚以及对机器人跌倒、碰撞、物体倾斜和超时等异常情况的严厉惩罚<br><img src="/images/MARL/high_level_RL.png" alt="high_level_RL"><br>中层控制器：去中心化MAPPO强化学习：机器狗自身观测o+子目标ah-&gt;am机器狗的目标速度向量，有一个比较特别的OCB reward&#x3D;<strong>v</strong>i ⋅ <strong>v</strong>target，机器狗到木箱最近点的向量、木箱到其子目标的向量<br><img src="/images/MARL/OCB_reward.png" alt="OCB_reward"><br>低层控制器：am-&gt;al机器狗电机控制，直接用了Unitree内置的<br>这个高层控制器的架构看上去有用，或许我也可以用RL对LLM生成的子目标做finetune?通过RL对LLM生成的建议作选择与优化</p>
<p><a target="_blank" rel="noopener" href="https://ml-jku.github.io/rudder/">RUDDER</a><br>将LSTM应用于reward-delayed RL的开创性文章(2018)，解决的问题是具有延迟奖励的强化学习环境动作价值函数具有高偏差，代理训练缓慢，解决方式是奖励重分配。<br>RUDDER 使用 LSTM 根据当前状态以及先前状态和操作的顺序预测每个时间步的最终累积奖励（回报），以确定哪些事件对最终奖励的贡献最大。一旦 LSTM 识别出导致最终奖励的重要事件，RUDDER 就会将外部奖励重新分配给那些关键时间步。文章有严谨的证明奖励重分配不改变游戏的总回报&#x2F;最优策略<br>问题是训练一个LSTM本身也需要时间，或许<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.09999">reward centering</a>已经是一个足够好的想法<br>ML-Agents也可以使用LSTM，但并不是RUDDER算法，而是DRQN：<a target="_blank" rel="noopener" href="https://github.com/miyamotok0105/unity-ml-agents/blob/master/docs/Feature-Memory.md">代理将能够存储浮点向量，以便在下次需要做出决策时使用</a></p>
<h3 id="经典多智能体算法"><a href="#经典多智能体算法" class="headerlink" title="- 经典多智能体算法"></a>- 经典多智能体算法</h3><p>主要介绍集中训练，分散执行的MARL算法</p>
<p>值分解类型：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.11485">QMIX</a><br>基于VDN算法改进，主要思想是优化一个Qtot网络(VDN的Qtot是Qi的线性加和，可想而知有很多问题)，Qtot输入当前环境观测τ,所有智能体的动作ai(以及每个智能体Qi网络的输出)，输出当前状态s下智能体做出联合动作a的预期回报，优化目标是趋近最优策略下(τ,a)的真实动作价值，其中τ is a joint actionobservation history，用RNN解决Dec-POMDP的环境部分可观测问题<br><img src="/images/MARL/QMIX.png" alt="QMIX"><br>文章假设给定环境观测τ，Qtot(τ,a)取最大值时，每个智能体Qi(τi,ai)一定取给定观测τi下的最大值，或者表示为 <strong>∂Qtot&#x2F;∂Qi&gt;&#x3D;0, ∀i&lt;&#x3D;N</strong><br><img src="/images/MARL/Qtot.png" alt="Qtot"><br>算法先通过group reward优化Qtot网络，再对Qtot求梯度反向传播就可以优化每个智能体的Q网络<br>算法假设环境中只有一个唯一的团队奖励，每个智能体没有自己独立的奖励函数</p>
<p>Actor-Crtic类型：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02275">MADDPG</a>，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53811876">知乎</a>这篇讲的已经很精炼了<br><img src="/images/MARL/MADDPG.png" alt="MADDPG"><br>MADDPG架构中，每个智能体都有自己的中心化价值网络和去中心化策略网络，允许每个智能体有独立的奖励函数，每个智能体的动作空间，观测空间也可以不一样</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.08926">COMA</a> 通过反事实推理&#x2F;差异奖励&#x2F;优势函数实现credit distribution，从而更新policy network<br>与MADDPG不同，COMA假设环境只存在group reward，并只使用一个中心化的Critic网络评估智能体联合动作的价值<br><img src="/images/MARL/COMA.png" alt="COMA"><br>优势函数为：<br><img src="/images/MARL/Advantage_function.png" alt="Advantage_function"><br>policy network更新通过：<br><img src="/images/MARL/COMA_policy_update.png" alt="COMA_policy_update"></p>
<p><a target="_blank" rel="noopener" href="https://rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf">MA-POCA</a>：Unity发表的多智能体算法，主要在COMA算法的基础上加上了自注意力机制。作者独到地注意到了现有多智能体算法无法处理可变多智能体总数的情况 <em>“ To the author’s knowledge, this is first time the posthumous credit assignment problem has been explicitly mentioned in the literature.”</em> ，并通过注意力机制将变长的集中观测、集中动作转化为了定长的特征向量，用作价值函数的输入。<br>算法假设了一个已知总智能体总数不超过N，但在每一时间步智能体数目不确定的环境(有新智能体产生或死亡)，死亡的智能体被视为无法做出动作以改变环境<br>为每个智能体i额外训练编码函数gi(oi),fi(oi,ai)，且两个编码函数输出的特征向量大小相同为d<br><img src="/images/MARL/POCA_center_critic.png" alt="POCA_center_critic"><br>反事实优势函数：<br><img src="/images/MARL/POCA_advantage.png" alt="POCA_advantage"><br>RSA(gj (oj), fi(oi,ai) i!&#x3D;j )返回了综合了oj和(oi,ai)信息的特征向量，大小仍为d，从而可以处理任意数量的代理联合动作</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.02511">LEMAE</a>和<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.03997">You Only LLM Once for MARL</a>的想法比较接近，通过对LLM chain-of-thought 式的提问引导，令LLM分离出任务的关键状态(子任务)，设计奖励函数等，目的是优化了ELLM需要反复询问LLM的问题，但相应的难以处理持续性任务</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">USER:</span><br><span class="line">(Task Description)</span><br><span class="line">Two agents are in a 30x30 room with the coordinate of the point in the upper left corner is</span><br><span class="line">(0,0) and the point in the lower right corner is (29,29), separated by a wall with a door.</span><br><span class="line">The door can not be directly open. There is a hidden switch in each room (The switch</span><br><span class="line">coordinates are unknown and differ from the coordinate of the door.). The door will be open</span><br><span class="line">only when one of the switch is occupied by an agent otherwise close. Both agents begin in</span><br><span class="line">the left half of the room(agent0.x&lt;15 and agent1.x&lt;15).</span><br><span class="line">The task is considered successfully completed as long as both agents are located in the right</span><br><span class="line">half-room(agent0.x&gt;15 and agent1.x&gt;15).</span><br><span class="line">(State Form)</span><br><span class="line">In each step we can get current state. The state is a 1-dim list:</span><br><span class="line">(agent0.x, agent0.y, agent1.x, agent1.y,door.open).</span><br><span class="line">(Role Instruction)(Template)</span><br><span class="line">Your role is to give several critical key states in the task...</span><br><span class="line">...</span><br><span class="line">ASSISTANT: (LLM response)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line">Key states: &#123;</span><br><span class="line">init:Initial state where both agents are in the left half of the room.,</span><br><span class="line">key state 1:Agent0 has found the switch and is standing on it, opening the door,</span><br><span class="line">key state 2:Agent1 has moved through the open door to the right half of the room while</span><br><span class="line">agent0 is still on the switch,</span><br><span class="line">key state 3:Agent0 has joined agent1 in the right half of the room</span><br><span class="line">&#125;,</span><br><span class="line">Discriminators: &#123;</span><br><span class="line">key state 1:def iskeystate1(state): return 1 if state[4] == 1 and state[0] &gt;15 else 0,</span><br><span class="line">key state 2:def iskeystate2(state): return 1 if state[4] == 1 and state[2] &gt;15 and state[0]</span><br><span class="line">&lt;15 else 0,</span><br><span class="line">key state 3:def iskeystate3(state): return 1 if state[0] &gt;15 and state[2] &gt;15 else 0</span><br><span class="line">&#125;,</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/images/MARL/co_pass.png" alt="co_pass"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/09/08/Reinforcement-Learning/" data-id="cm45fdq7n0000gsk31qsj4gnc" data-title="Reinforcement Learning" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/09/02/Multi-Agents/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Multi Agents</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/09/08/Reinforcement-Learning/">Reinforcement Learning</a>
          </li>
        
          <li>
            <a href="/2024/09/02/Multi-Agents/">Multi Agents</a>
          </li>
        
          <li>
            <a href="/2024/07/28/ROS2-Learning/">ROS2_Learning</a>
          </li>
        
          <li>
            <a href="/2024/07/28/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>