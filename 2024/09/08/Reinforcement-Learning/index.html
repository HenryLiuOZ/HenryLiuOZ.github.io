<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Reinforcement Learning | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="此博客主要介绍强化学习算法，相关论文也会放在里面 Chapter1：强化学习背景与基础知识参考：huggingfaceWindyLab 单智能体强化学习旨在解决一类马尔可夫决策过程 (马尔可夫性：环境状态转移的概率仅依赖于当前状态和动作) 的问题。强化学习的假设是代理能够在一个静态环境 environment with stationary distribution （状态转移概率和奖励函数是恒定">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning">
<meta property="og:url" content="http://example.com/2024/09/08/Reinforcement-Learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="此博客主要介绍强化学习算法，相关论文也会放在里面 Chapter1：强化学习背景与基础知识参考：huggingfaceWindyLab 单智能体强化学习旨在解决一类马尔可夫决策过程 (马尔可夫性：环境状态转移的概率仅依赖于当前状态和动作) 的问题。强化学习的假设是代理能够在一个静态环境 environment with stationary distribution （状态转移概率和奖励函数是恒定">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/MARL/Rint_define.png">
<meta property="og:image" content="http://example.com/images/MARL/ELLMprocess.png">
<meta property="og:image" content="http://example.com/images/Multi-Agents/Eureka1.png">
<meta property="article:published_time" content="2024-09-08T09:05:10.000Z">
<meta property="article:modified_time" content="2024-12-17T10:05:04.857Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/MARL/Rint_define.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/09/08/Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2024-09-08T09:05:10.000Z" itemprop="datePublished">2024-09-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Reinforcement Learning
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>此博客主要介绍强化学习算法，相关论文也会放在里面</p>
<h2 id="Chapter1：强化学习背景与基础知识"><a href="#Chapter1：强化学习背景与基础知识" class="headerlink" title="Chapter1：强化学习背景与基础知识"></a>Chapter1：强化学习背景与基础知识</h2><p>参考：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/deep-rl-course/unit1/what-is-rl">huggingface</a><br><a target="_blank" rel="noopener" href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning">WindyLab</a></p>
<p>单智能体强化学习旨在解决一类马尔可夫决策过程 (马尔可夫性：环境状态转移的概率仅依赖于当前状态和动作) 的问题。强化学习的假设是代理能够在一个静态环境 <em>environment with stationary distribution</em> （状态转移概率和奖励函数是恒定的）中通过与环境的交互寻找最大化奖励的行为。<br>在单智能体强化学习中，智能体基于 <strong>当前环境</strong> 采取某些action并与环境进行交互，这些动作可能会改变环境的状态，环境根据人为定义的奖励函数向智能体反馈奖励r。强化学习算法以优化到 <strong>贝尔曼最优公式</strong> 为目标，寻找最优策略π∗(a|s)  </p>
<hr>
<p>基本概念：</p>
<ul>
<li>state transition probability: $p(s’|s,a)$</li>
<li>reward function: $r(s,a)$</li>
<li>policy function: $\pi(a|s)$, the only variable while training</li>
<li>state value:<br>$v_{\pi}(s)&#x3D;E[\sum_{t&#x3D;0}^{\infty} \gamma^{t}R_{t+1}|S_t&#x3D;s]$<br>$R_{t+1}$ is the immediate reward following current policy<br>matrix form: $v_{\pi}&#x3D;r_{\pi}+\gamma P_{\pi}v_{\pi}$</li>
<li>action value: $v_{\pi}(s)&#x3D;\sum_a \pi(a|s)q_{\pi}(s)$</li>
<li>Bellman equation: $v_{\pi}(s)&#x3D;\sum_a \pi(a|s)(\sum_r p(r|s,a)*r+\sum_{s’}p(s’|s,a)v_{\pi}(s’))$<br>matrix form: $v_{\pi}&#x3D;r_{\pi}+\gamma P_{\pi}v_{\pi}$</li>
<li>Bellman Optimal Equation: for any policy $\pi$ and state s, $v_{\pi*}(s)\geq v_{\pi}(s)$<br>matrix form: $v&#x3D;f(v)&#x3D;{max}<em>{\pi}$ $(r</em>{\pi}+\gamma v)$</li>
</ul>
<hr>
<p><a target="_blank" rel="noopener" href="https://github.com/HenryLiuOZ/Reinforcement-Learning-Notes">my RL notes</a></p>
<h2 id="Chapter2：内在动机强化学习算法（IMRL）"><a href="#Chapter2：内在动机强化学习算法（IMRL）" class="headerlink" title="Chapter2：内在动机强化学习算法（IMRL）"></a>Chapter2：内在动机强化学习算法（IMRL）</h2><p><a target="_blank" rel="noopener" href="https://github.com/yuqingd/ellm?tab=readme-ov-file">Guiding Pretraining in Reinforcement Learning with Large Language Models</a></p>
<p>实际问题：”When reward functions are sparse, agents often need to carry out a long, specific sequence of actions to achieve target tasks. As action spaces or target behaviors grow more complex, the space of alternative action sequences agents can explore grows combnatorially. In such scenarios, undirected exploration that randomly perturbs actions or policy parameters has littlechance of succeeding” 代理在奖励稀疏环境中训练困难，人为设计频繁精细的奖励费时费力</p>
<p>初步解决-IMRL：”Many distinct action sequences can lead to similar outcomes (Baranes &amp; Oudeyer, 2013)—for example, most action sequences cause a humanoid agent to fall, while very few make it walk. Building on this observation, intrinsically motivated RL algorithms (IM-RL) choose to explore outcomes rather than actions” IMRL基于新颖的行为结果为代理提供内在奖励从而促使代理做出不同的探索。而仅仅是探索更新奇的状态可能会促使代理沉浸在与最终目标无关的行为中。Competence-based Intrinsic Motivation基于评估代理 <em>掌握新的技能</em> 给出奖励<br>“The approach we introduce in this paper, ELLM, may be interpreted as a CB-IM algorithm that seeks to explore the space of possible and plausibly useful skills informed by human prior knowledge.”</p>
<h3 id="CB-IM"><a href="#CB-IM" class="headerlink" title="CB-IM"></a>CB-IM</h3><p>“A CB-IM agent is expected to perform well with respect to the original R when the intrinsic reward <strong>Rint(o,a,o’|g)</strong> is both easier to optimize and well aligned with R, such that behaviors maximizing Rint also maximize R.” 这个Rint和goal需要手动设计 “Most CB-IM algorithms hand-define the reward functions Rint and the support of the goal distribution in alignment with the original task R, but use various intrinsic motivations to guide goal sampling (1): e.g.novelty, learning progress, intermediate difficulty” 而ELLM使用大语言模型自动生成目标并评估代理行为</p>
<h3 id="ELLM"><a href="#ELLM" class="headerlink" title="ELLM"></a>ELLM</h3><p><img src="/images/MARL/Rint_define.png" alt="Rint define"><br><img src="/images/MARL/ELLMprocess.png" alt="ELLMprocess"><br>“We consider two forms of agent training: (1) a goal conditioned setting where the agent is given a sentence embedding of the list of suggested goals, π(a | o,E(g1:k)), and (2) a goal-free setting where the agent does not have access to the suggested goals, π(a | o). While Rint remains the same in either case, training a goal-conditioned agent introduces both challenges and benefits: <strong>it can take time for the agent to learn the meaning of the different goals and connect it to the reward</strong>, but having a language-goal conditioned policy can be moreamenable to downstream tasks than an agent just trained on an exploration reward.”</p>
<h3 id="论文比较-Eureka"><a href="#论文比较-Eureka" class="headerlink" title="论文比较-Eureka"></a>论文比较-Eureka</h3><p>都是设计奖励函数以解决环境奖励稀疏问题，<a target="_blank" rel="noopener" href="https://github.com/eureka-research/Eureka?tab=readme-ov-file">Eureka</a>与ELLM有何不同？<br><img src="/images/Multi-Agents/Eureka1.png" alt="baseline"><br>相比之下，Eureka更加“黑盒”。ELLM是带有逻辑性的(如果你承认LLM有逻辑的话)，它根据游戏规则为代理给出建议，并鼓励代理达成这些目标；而Eureka是纯粹的奖励设计，它先生成一批奖励函数，再在环境中训练代理，挑选那些能让代理训练效果更好的奖励函数，并不断迭代</p>
<h3 id="ELLM-recurrence"><a href="#ELLM-recurrence" class="headerlink" title="ELLM recurrence"></a>ELLM recurrence</h3><p><a target="_blank" rel="noopener" href="https://gitmind.com/app/docs/mnobpjvz">my code mind map for ellm</a><br>可惜的是，ellm并没有做任何的可视化，测试员唯一能看到的就是训练进程的tensorboard或者最终生成的stats.jsonl文件<br>ellm主要传递给agent的observation是游戏环境的text描述+对text_obs用sentence-transformer tokenize化</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_obs</span>(<span class="params">self, obs_dict</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  Takes in obs dict and returns a dict where all strings are tokenized.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> <span class="variable language_">self</span>.use_sbert <span class="keyword">and</span> <span class="built_in">isinstance</span>(obs_dict[<span class="string">&#x27;inv_status&#x27;</span>], <span class="built_in">dict</span>):</span><br><span class="line">      inv_status = <span class="string">&quot;&quot;</span></span><br><span class="line">      <span class="keyword">for</span> k, v <span class="keyword">in</span> obs_dict[<span class="string">&#x27;inv_status&#x27;</span>].items():</span><br><span class="line">          <span class="keyword">if</span> v != <span class="string">&#x27;.&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;null&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> v:</span><br><span class="line">              inv_status += v + <span class="string">&quot; &quot;</span></span><br><span class="line">      obs_dict[<span class="string">&#x27;text_obs&#x27;</span>] = obs_dict[<span class="string">&#x27;text_obs&#x27;</span>] + <span class="string">&quot; &quot;</span> + inv_status</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_obs</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a dictionary of text observations&quot;&quot;&quot;</span></span><br><span class="line">    inv, status = <span class="variable language_">self</span>._inventory_to_text()</span><br><span class="line">    obs = <span class="variable language_">self</span>._text_view.local_sentence_view(<span class="variable language_">self</span>.player)</span><br><span class="line">    <span class="keyword">return</span> obs.lower(), &#123;<span class="string">&#x27;inv&#x27;</span> : inv.lower(), <span class="string">&#x27;status&#x27;</span>: status.lower()&#125;</span><br></pre></td></tr></table></figure>

<p>tokenize_obs()主要将当前的text_obs()一串字符串使用sentence-transformer转换为一串值（方便神经网络学习）</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EmbeddingView</span>(<span class="title class_ inherited__">SemanticView</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, world, obj_types, grid</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__(world, obj_types)</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._grid = np.array(grid)</span><br><span class="line">    <span class="variable language_">self</span>._offset = <span class="variable language_">self</span>._grid // <span class="number">2</span></span><br><span class="line">    <span class="variable language_">self</span>._area = np.array(<span class="variable language_">self</span>._world.area)</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._names = &#123;value : key <span class="keyword">for</span> (key, value) <span class="keyword">in</span> <span class="variable language_">self</span>._mat_ids.items()&#125;</span><br><span class="line">    <span class="variable language_">self</span>._obj_names = &#123;value : key.__name__ <span class="keyword">for</span> (key, value) <span class="keyword">in</span> <span class="variable language_">self</span>._obj_ids.items()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>._names.update(<span class="variable language_">self</span>._obj_names)</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_view</span>(<span class="params">self, player_pos</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Return local view of semantic array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    center = np.array(player_pos)</span><br><span class="line">    canvas = <span class="variable language_">self</span>._world._mat_map.copy()</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> <span class="variable language_">self</span>._world.objects:</span><br><span class="line">      canvas[<span class="built_in">tuple</span>(obj.pos)] = <span class="variable language_">self</span>._obj_ids[<span class="built_in">type</span>(obj)]</span><br><span class="line">    local_canvas = np.zeros(<span class="variable language_">self</span>._grid)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>._grid[<span class="number">0</span>]):</span><br><span class="line">      <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>._grid[<span class="number">1</span>]):</span><br><span class="line">        pos = center + np.array([x, y]) - <span class="variable language_">self</span>._offset</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> _inside((<span class="number">0</span>, <span class="number">0</span>), pos, <span class="variable language_">self</span>._area):</span><br><span class="line">          <span class="keyword">continue</span> </span><br><span class="line">        local_canvas[x,y] = canvas[<span class="built_in">tuple</span>(pos)]</span><br><span class="line">    <span class="keyword">return</span> local_canvas</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_token_view</span>(<span class="params">self, player</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      Return a token representation of the observations.</span></span><br><span class="line"><span class="string">      [[&quot;grass&quot;, &quot;grass&quot;, &quot;tree&quot;],</span></span><br><span class="line"><span class="string">       [&quot;grass, &quot;player&quot;, &quot;grass&quot;],</span></span><br><span class="line"><span class="string">       ...] </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    local_canvas = <span class="variable language_">self</span>.local_view(player.pos)</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Right now we replace each index with the corresponding string. </span></span><br><span class="line">    <span class="comment"># In the future, we should place in word embedding directly.</span></span><br><span class="line">    name_canvas = []</span><br><span class="line">    <span class="keyword">for</span> x, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(local_canvas): </span><br><span class="line">      name_canvas.append([])</span><br><span class="line">      <span class="keyword">for</span> y, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(row):</span><br><span class="line">        name = <span class="variable language_">self</span>._names[idx]</span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          name = <span class="string">&#x27;Null&#x27;</span></span><br><span class="line">        name_canvas[x].append(name)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> name_canvas</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">local_sentence_view</span>(<span class="params">self, player</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      Return a sentence representation of the observations.</span></span><br><span class="line"><span class="string">      &quot;You see cows, trees, and grass&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    local_canvas = <span class="variable language_">self</span>.local_view(player.pos)</span><br><span class="line">    sentence_obs = <span class="string">&quot;You see &quot;</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>而当前环境的text_obs用了每个gameobject的编号来代替它本身在游戏中的像素点，所以取出来也是一个整数矩阵，算是一个取巧的方法，但可惜这个游戏就失去了可视化界面了</p>
<p>回到我最开始的研究思路：当前ellm是通过LM为代理提供建议，并奖励代理达成这些建议，我的想法是为什么不直接让LM生成一条路径，直接 <em>操纵代理去执行这条动作路径</em> ，这样也免去了代理需要学习做出动作和对应建议之间关系的训练时间<br><strong>ellm和轨迹生成的区别实际很小</strong>：我当时以为ellm生成的建议依旧是需要多个时间步以达成的建议，比如建议代理现在做一张桌子，可能需要先去砍树，再合成；事实上ellm采用的crafter环境是一个过于简单的游戏环境，建议是在每一个时间步生成，并且保证每一个时间步都能达成一个建议（建议实际上就是一个action，这和生成一个trajectory也没什么区别了）<br><strong>我当前的研究思路</strong>：将ellm的方法运用到我的FRCThree环境中，我可以像ellm一样，先生成每一步的动作建议，再计算当前动作和上一次建议之间的相似度；也可以手动实现一些函数（如move_a_step_to）并且让LM在每一步选择这些函数并调用（<em>操纵代理去执行这条动作路径</em>），有没有觉得这有点像宋学长上个星期的汇报？<br>更进一步，我认为当前的算法不足以解决3v3的多智能体问题（我们能保证LM同时给三个机器人提供建议不会造成冲突吗，比如，同时建议他们三个去拿一个NOTE？），我近期再看一些多智能体算法的文章，整理一下自己的思路，争在1月份前确定自己最终的算法结构</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/09/08/Reinforcement-Learning/" data-id="cm45fdq7n0000gsk31qsj4gnc" data-title="Reinforcement Learning" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/09/02/Multi-Agents/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Multi Agents</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/09/08/Reinforcement-Learning/">Reinforcement Learning</a>
          </li>
        
          <li>
            <a href="/2024/09/02/Multi-Agents/">Multi Agents</a>
          </li>
        
          <li>
            <a href="/2024/07/28/ROS2-Learning/">ROS2_Learning</a>
          </li>
        
          <li>
            <a href="/2024/07/28/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>